{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating HDF5 data\n",
    "\n",
    "Caffe takes inputs in HDF5, database(LevelDB/LMDB) and image(JPG) formats. In this demo, we show how to prepare HDF5 data for Caffe consumption! \n",
    "\n",
    "We follow the same steps that we followed in HW0 to load CIFAR-10 data into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the usual startup! \n",
    "import numpy as np\n",
    "from get_cifar10 import load_CIFAR10\n",
    "\n",
    "cifar10_dir = '/path/to/hw1/1_cs231n/cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Subsample the data for more efficient code execution in this exercise.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# caffe needs your data to be in float32\n",
    "X_train = np.array(X_train,dtype = 'float32')\n",
    "y_train = np.array(y_train,dtype = 'float32')\n",
    "X_val = np.array(X_val,dtype = 'float32')\n",
    "y_val = np.array(y_val,dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And, before we feed the data to the classifier, we need to subtract the mean! \n",
    "mean_image = np.mean(X_train,axis = 0)\n",
    "print mean_image.shape\n",
    "\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "\n",
    "print X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you have read the Caffe tutorial, you will know that Caffe expects input\n",
    "# in the form of 4-dimensional blobs\n",
    "# N_images X N_channels X Height X Width\n",
    "# So, let us reshape them! \n",
    "\n",
    "num_channels = 3\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "print X_val.shape\n",
    "\n",
    "X_train = np.reshape(X_train, (num_training,num_channels,height,width))\n",
    "X_val = np.reshape(X_val, (num_validation,num_channels,height,width))\n",
    "\n",
    "# Check the data shape again! \n",
    "print 'Modified Train data shape: ', X_train.shape\n",
    "print 'Modified Validation data shape: ', X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# proceed to creating HDF5. We will create and store the data in folder `hdf5-data` folder \n",
    "import h5py\n",
    "# with h5py.Fil('/path/to/whatever-name-you-want.h5','w') as f:\n",
    "with h5py.File('hdf5-data/train.h5', 'w') as f:\n",
    "    f['images'] = X_train # f['name'] - the name field is very important as that is what caffe will recognize\n",
    "    f['labels'] = y_train\n",
    "with h5py.File('hdf5-data/test.h5', 'w') as f:\n",
    "    f['images'] = X_val\n",
    "    f['labels'] = y_val\n",
    "print 'HDF5 files are written. Need to make the txt files!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the `hdf5-data` folder, you need to create two files `train.txt` and `test.txt` that have the path to the corresponding hdf5 file. As part of this example, these files are provided to you `hdf5-data` folfer. Open these files and make appropriate changes to the path. In case you changed the names of the hdf5 files, you need to change the `.txt` files also before proceeding to training.\n",
    "\n",
    "Before starting the training process:\n",
    "- make approporiate changes in the `run_logreg.sh` and `logreg_solver.prototxt` files in the path fields. \n",
    "- create a folder for the snapshots to be stored as mentioned against the `snapshot-prefix` field in the solver file\n",
    "\n",
    "Run the `run_logreg.sh` file to start the training. \n",
    "\n",
    "Note that for this particular example, the details in the solver file are suited for the softmax classifier alone. For the other problems part of this assignment, a solver file that looks similar is provided. You need to make appropriate changes to obtain better results. You could also try new strategies for altering learning rate, new solver methods, etc. to improve your training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming that you have completed training the classifer, let us plot the training loss vs. iteration. This is an\n",
    "# example to show the usefullness of logging data. Setting debug_info = 1 in the solver file creates verbose logs\n",
    "# that can be useful in debugging the network. \n",
    "\n",
    "# we neeed matplotlib to plot the graphs for us!\n",
    "import matplotlib\n",
    "# This is needed to save images \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we need to put the log parsing code in the python path\n",
    "import sys\n",
    "sys.path.insert(0,'/path/to/caffe/tools/extra/')\n",
    "\n",
    "from parse_log import parse_log\n",
    "\n",
    "train_log, test_log = parse_log('/path/to/hw1/2_caffe/logreg/logreg.log')\n",
    "\n",
    "# view the extracted data\n",
    "# print train_log\n",
    "#print test_log\n",
    "\n",
    "# extract the required fields for plotting \n",
    "iters = []\n",
    "loss = []\n",
    "# test_interval should be the same as the number of iterations you display so that\n",
    "# for every entry in iters you have a testing accuracy value. If different, make suitable\n",
    "# modifications based on len(test_log)\n",
    "accuracy = []\n",
    "for i in range(len(train_log)):\n",
    "    iters.append(train_log[i]['NumIters'])\n",
    "    loss.append(train_log[i]['loss'])\n",
    "    accuracy.append(test_log[i]['accuracy']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "# If your hyper-parameters are tuned, it should decrease exponentially with iterations. \n",
    "plt.close()\n",
    "plt.plot(iters,loss)\n",
    "# saves iters vs. loss \n",
    "plt.savefig('logreg/logreg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "# if your hyper-parameters are tuned, it should hit a plateau at ~35% accuracy. \n",
    "plt.close()\n",
    "plt.plot(iters,accuracy) \n",
    "# save figure\n",
    "plt.savefig('logreg/logreg_accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
